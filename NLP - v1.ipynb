{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de21d357",
   "metadata": {},
   "source": [
    "# Analysing the abstract of - On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? - using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c1f7a9",
   "metadata": {},
   "source": [
    "### Loading in libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d655a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Punctuation removal\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5599ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = \"The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dc8aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5be284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English.',\n",
       " 'BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size.',\n",
       " 'Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English.',\n",
       " 'In this paper, we take a step back and ask: How big is too big?',\n",
       " 'What are the possible risks associated with this technology and what paths are available for mitigating those risks?',\n",
       " 'We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(abstract)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f81d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models especially for English', 'BERT its variants GPT 2 3 and others most recently Switch C have pushed the boundaries of the possible both through architectural innovations and through sheer size', 'Using these pretrained models and the methodology of fine tuning them for specific tasks researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English', 'In this paper we take a step back and ask How big is too big', 'What are the possible risks associated with this technology and what paths are available for mitigating those risks', 'We provide recommendations including weighing the environmental and financial costs first investing resources into curating and carefully documenting datasets rather than ingesting everything on the web carrying out pre development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values and encouraging research directions beyond ever larger language models']\n"
     ]
    }
   ],
   "source": [
    "# Only returning characters that is not a letter (a-zA-Z), digit (0-9), or whitespace (\\s).\n",
    "clean_sentences = [re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", sentence) for sentence in sentences]\n",
    "\n",
    "# Remove multiple spaces and strip leading/trailing spaces\n",
    "clean_sentences = [re.sub(r\"\\s+\", \" \", sentence).strip() for sentence in clean_sentences]\n",
    "\n",
    "# Output cleaned sentences - each sentence is an individual string in the list\n",
    "print(clean_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92149de",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d82be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975b5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'past', '3', 'years', 'of', 'work', 'in', 'NLP', 'have', 'been', 'characterized', 'by', 'the', 'development', 'and', 'deployment', 'of', 'ever', 'larger', 'language', 'models', 'especially', 'for', 'English', 'BERT', 'its', 'variants', 'GPT', '2', '3', 'and', 'others', 'most', 'recently', 'Switch', 'C', 'have', 'pushed', 'the', 'boundaries', 'of', 'the', 'possible', 'both', 'through', 'architectural', 'innovations', 'and', 'through', 'sheer', 'size', 'Using', 'these', 'pretrained', 'models', 'and', 'the', 'methodology', 'of', 'fine', 'tuning', 'them', 'for', 'specific', 'tasks', 'researchers', 'have', 'extended', 'the', 'state', 'of', 'the', 'art', 'on', 'a', 'wide', 'array', 'of', 'tasks', 'as', 'measured', 'by', 'leaderboards', 'on', 'specific', 'benchmarks', 'for', 'English', 'In', 'this', 'paper', 'we', 'take', 'a', 'step', 'back', 'and', 'ask', 'How', 'big', 'is', 'too', 'big', 'What', 'are', 'the', 'possible', 'risks', 'associated', 'with', 'this', 'technology', 'and', 'what', 'paths', 'are', 'available', 'for', 'mitigating', 'those', 'risks', 'We', 'provide', 'recommendations', 'including', 'weighing', 'the', 'environmental', 'and', 'financial', 'costs', 'first', 'investing', 'resources', 'into', 'curating', 'and', 'carefully', 'documenting', 'datasets', 'rather', 'than', 'ingesting', 'everything', 'on', 'the', 'web', 'carrying', 'out', 'pre', 'development', 'exercises', 'evaluating', 'how', 'the', 'planned', 'approach', 'fits', 'into', 'research', 'and', 'development', 'goals', 'and', 'supports', 'stakeholder', 'values', 'and', 'encouraging', 'research', 'directions', 'beyond', 'ever', 'larger', 'language', 'models']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words for each cleaned sentence and flatten the list\n",
    "words = [word for sentence in clean_sentences for word in word_tokenize(sentence)]\n",
    "\n",
    "# Output the list of words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac6516",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff7de76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1a65fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'past', '3', 'years', 'work', 'NLP', 'characterized', 'development', 'deployment', 'ever', 'larger', 'language', 'models', 'especially', 'English', 'BERT', 'variants', 'GPT', '2', '3', 'others', 'recently', 'Switch', 'C', 'pushed', 'boundaries', 'possible', 'architectural', 'innovations', 'sheer', 'size', 'Using', 'pretrained', 'models', 'methodology', 'fine', 'tuning', 'specific', 'tasks', 'researchers', 'extended', 'state', 'art', 'wide', 'array', 'tasks', 'measured', 'leaderboards', 'specific', 'benchmarks', 'English', 'In', 'paper', 'take', 'step', 'back', 'ask', 'How', 'big', 'big', 'What', 'possible', 'risks', 'associated', 'technology', 'paths', 'available', 'mitigating', 'risks', 'We', 'provide', 'recommendations', 'including', 'weighing', 'environmental', 'financial', 'costs', 'first', 'investing', 'resources', 'curating', 'carefully', 'documenting', 'datasets', 'rather', 'ingesting', 'everything', 'web', 'carrying', 'pre', 'development', 'exercises', 'evaluating', 'planned', 'approach', 'fits', 'research', 'development', 'goals', 'supports', 'stakeholder', 'values', 'encouraging', 'research', 'directions', 'beyond', 'ever', 'larger', 'language', 'models']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5493126",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e26be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\smorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7037b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'past', '3', 'year', 'work', 'nlp', 'character', 'develop', 'deploy', 'ever', 'larger', 'languag', 'model', 'especi', 'english', 'bert', 'variant', 'gpt', '2', '3', 'other', 'recent', 'switch', 'c', 'push', 'boundari', 'possibl', 'architectur', 'innov', 'sheer', 'size', 'use', 'pretrain', 'model', 'methodolog', 'fine', 'tune', 'specif', 'task', 'research', 'extend', 'state', 'art', 'wide', 'array', 'task', 'measur', 'leaderboard', 'specif', 'benchmark', 'english', 'in', 'paper', 'take', 'step', 'back', 'ask', 'how', 'big', 'big', 'what', 'possibl', 'risk', 'associ', 'technolog', 'path', 'avail', 'mitig', 'risk', 'we', 'provid', 'recommend', 'includ', 'weigh', 'environment', 'financi', 'cost', 'first', 'invest', 'resourc', 'curat', 'care', 'document', 'dataset', 'rather', 'ingest', 'everyth', 'web', 'carri', 'pre', 'develop', 'exercis', 'evalu', 'plan', 'approach', 'fit', 'research', 'develop', 'goal', 'support', 'stakehold', 'valu', 'encourag', 'research', 'direct', 'beyond', 'ever', 'larger', 'languag', 'model']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceb04a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smorr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f7bd386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'past', '3', 'year', 'work', 'NLP', 'characterized', 'development', 'deployment', 'ever', 'larger', 'language', 'model', 'especially', 'English', 'BERT', 'variant', 'GPT', '2', '3', 'others', 'recently', 'Switch', 'C', 'pushed', 'boundary', 'possible', 'architectural', 'innovation', 'sheer', 'size', 'Using', 'pretrained', 'model', 'methodology', 'fine', 'tuning', 'specific', 'task', 'researcher', 'extended', 'state', 'art', 'wide', 'array', 'task', 'measured', 'leaderboards', 'specific', 'benchmark', 'English', 'In', 'paper', 'take', 'step', 'back', 'ask', 'How', 'big', 'big', 'What', 'possible', 'risk', 'associated', 'technology', 'path', 'available', 'mitigating', 'risk', 'We', 'provide', 'recommendation', 'including', 'weighing', 'environmental', 'financial', 'cost', 'first', 'investing', 'resource', 'curating', 'carefully', 'documenting', 'datasets', 'rather', 'ingesting', 'everything', 'web', 'carrying', 'pre', 'development', 'exercise', 'evaluating', 'planned', 'approach', 'fit', 'research', 'development', 'goal', 'support', 'stakeholder', 'value', 'encouraging', 'research', 'direction', 'beyond', 'ever', 'larger', 'language', 'model']\n"
     ]
    }
   ],
   "source": [
    "# Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a1910",
   "metadata": {},
   "source": [
    "### Topic modelling - using LDA (Latent Dirichlet Allocation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d278dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\smorr\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\smorr\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.1)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.6.2\n",
      "    Uninstalling scipy-1.6.2:\n",
      "      Successfully uninstalled scipy-1.6.2\n",
      "Successfully installed gensim-4.3.3 scipy-1.10.1 smart-open-7.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43da7040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.011*\"development\" + 0.011*\"model\" + 0.011*\"3\" + 0.011*\"language\"')\n",
      "(1, '0.022*\"model\" + 0.022*\"development\" + 0.016*\"English\" + 0.016*\"big\"')\n"
     ]
    }
   ],
   "source": [
    "documents = [lemmatized]  # Wrap it in a list to create a single document\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Create a corpus: List of bag-of-words representations\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "# Perform LDA\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187d378",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c952b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0deb1093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models especially for English\n",
      "Sentiment: Sentiment(polarity=-0.0625, subjectivity=0.4375)\n",
      "Sentence: BERT its variants GPT 2 3 and others most recently Switch C have pushed the boundaries of the possible both through architectural innovations and through sheer size\n",
      "Sentiment: Sentiment(polarity=0.125, subjectivity=0.625)\n",
      "Sentence: Using these pretrained models and the methodology of fine tuning them for specific tasks researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English\n",
      "Sentiment: Sentiment(polarity=0.06333333333333332, subjectivity=0.22999999999999998)\n",
      "Sentence: In this paper we take a step back and ask How big is too big\n",
      "Sentiment: Sentiment(polarity=0.0, subjectivity=0.06666666666666667)\n",
      "Sentence: What are the possible risks associated with this technology and what paths are available for mitigating those risks\n",
      "Sentiment: Sentiment(polarity=0.2, subjectivity=0.7)\n",
      "Sentence: We provide recommendations including weighing the environmental and financial costs first investing resources into curating and carefully documenting datasets rather than ingesting everything on the web carrying out pre development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values and encouraging research directions beyond ever larger language models\n",
      "Sentiment: Sentiment(polarity=0.0375, subjectivity=0.4583333333333333)\n"
     ]
    }
   ],
   "source": [
    "# Analyze sentiment\n",
    "for sentence in clean_sentences:\n",
    "    blob = TextBlob(sentence)\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {blob.sentiment}\")  # returns a tuple (polarity, subjectivity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
